{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "xZP7nP6unG_t"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Imports PIL module\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "t0Wuhlg1TH95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device being used: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)  # Seed for NumPy random number generator\n",
    "torch.manual_seed(RANDOM_SEED)  # Seed for PyTorch random number generator\n",
    "\n",
    "# Set the depth of the network\n",
    "depth = 32  # Initial depth of the Siamese network\n",
    "img_channels = 3  # Number of color channels (3 for RGB)\n",
    "img_w = 172  # Image width\n",
    "img_h = 128  # Image height\n",
    "\n",
    "# Determine the computing device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'device being used: {device}')\n",
    "\n",
    "# Configuration settings\n",
    "n_classes = 81 # Number of identities in the dataset - range [0, 80]\n",
    "n_epochs = 50  # Number of training epochs\n",
    "train_batch_size = 32  # Batch size for training\n",
    "val_batch_size = 1  # Batch size for validation\n",
    "test_batch_size = 1  # Batch size for testing\n",
    "learning_rate = 0.0005  # Learning rate for the optimizer\n",
    "backbone = 'VGG19'  # Backbone network architecture ['siameseNet', 'VGG16', 'VGG19', 'denseNet', 'MobileNetV3', 'efficientNetB0', 'ViT16']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gwbJQqPYwIv"
   },
   "source": [
    "## Load Probe and Gallery sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ID\n",
    "def extract_id_from_imagename(filename):\n",
    "    \"\"\" Extract the ID from the filename, including the number and the trailing 'A' or 'B'. \"\"\"\n",
    "    #match = re.search(r'_(\\d+)[mf]?([AB])\\_', filename) # e.g., 00001A\n",
    "    #return f\"{match.group(1)}{match.group(2)}\" if match else None\n",
    "    match = re.search(r'(\\d+[mf][AB])\\_', filename) # e.g., 00001mA\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "# Define a custom dataset class for training, specific to handling probe and gallery images\n",
    "class ProbeGalleryTrain(Dataset):\n",
    "    def __init__(self, data_root, transform=None):\n",
    "        # Initialize the dataset with the root directory of the data and any transformations to be applied\n",
    "        self.data_root = data_root  # Store the directory where the dataset is located\n",
    "        self.data = self.read_folder()  # Read image paths and labels from the directory\n",
    "        self.samples = self.create_samples()  # Create matched and unmatched pairs of images (samples)\n",
    "        self.transform = transform  # Store the transformations to be applied to each image\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of image pairs in the dataset\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Fetch a specific sample by its index\n",
    "        im1_pth, im2_pth, is_match, probe_target, gallery_target = self.samples[idx]  # Extract details of the indexed pair\n",
    "        im1 = Image.open(im1_pth)  # Load the first image\n",
    "        im2 = Image.open(im2_pth)  # Load the second image\n",
    "\n",
    "        if self.transform:\n",
    "            # Apply specified transformations to both images, if any\n",
    "            im1 = self.transform(im1)\n",
    "            im2 = self.transform(im2)\n",
    "\n",
    "        # Return the pair of images along with their match status and associated targets\n",
    "        return [im1, im2, is_match, probe_target, gallery_target]\n",
    "\n",
    "    def read_folder(self):\n",
    "        # Read and store the file paths and labels of images from the dataset directory\n",
    "        paths = []  # List to hold paths of images\n",
    "        labels = []  # List to hold corresponding labels ('Probe' or 'Gallery')\n",
    "\n",
    "        # Traverse the dataset directory\n",
    "        for dirpath, dirnames, filenames in os.walk(self.data_root):\n",
    "            # Select only image files (.jpg, .jpeg, .png)\n",
    "            files = [f for f in filenames if f.split('.')[-1] in ['jpg', 'jpeg', 'png']]\n",
    "            for item in files:\n",
    "                # Extract the label (directory name) and append both path and label to the lists\n",
    "                label = dirpath.split('/')[-1]\n",
    "                paths.append([os.path.join(dirpath, item), label])\n",
    "                if label not in labels:\n",
    "                    labels.append(label)\n",
    "\n",
    "        return paths\n",
    "\n",
    "    def create_samples(self):\n",
    "        # Generate pairs of images (probes paired with each gallery image) with matching status\n",
    "        probes = [x[0] for x in self.data if x[1] == 'Probe']  # Extract paths of 'Probe' images\n",
    "        gallery = [x[0] for x in self.data if x[1] == 'Gallery']  # Extract paths of 'Gallery' images\n",
    "\n",
    "        samples = []\n",
    "        # Create pairs of probe and gallery images, and determine if they match\n",
    "        for probe in probes:\n",
    "            probe_id = extract_id_from_imagename(probe)\n",
    "            probe_id = int(probe_id[:-2])-1 # IDs range [1, 81] -> [0, 80]\n",
    "            for item in gallery:\n",
    "                item_id = extract_id_from_imagename(item)\n",
    "                item_id = int(item_id[:-2])-1 # IDs range [1, 81] -> [0, 80]\n",
    "                # Match status: 1 if IDs are similar (ignoring the last two characters), else -1\n",
    "                samples.append([probe, item, 1 if probe_id == item_id else -1, probe_id, item_id])\n",
    "\n",
    "        return samples\n",
    "\n",
    "\n",
    "# create the val/test dataset\n",
    "class ProbeGalleryValTest(Dataset):\n",
    "    def __init__(self, data_root, transform=None):\n",
    "        # Initialize the dataset object with the root directory of the data and optional transformations\n",
    "        self.data_root = data_root  # Store the root directory where the data is located\n",
    "        self.data = self.read_folder()  # Read the data from the folder\n",
    "        self.samples = self.create_samples()  # Create samples from the data\n",
    "        self.transform = transform  # Store any transformations to be applied to the images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)  # Return the number of samples in the dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      # Retrieve the sample data at the given index 'idx'\n",
    "      probe_pth, gallery_pths, is_match, probe_target, gallery_targets = self.samples[idx]  # Get paths and target labels\n",
    "\n",
    "      # Load the probe image from its path\n",
    "      probe = Image.open(probe_pth)  # Open the probe image file\n",
    "\n",
    "      # Initialize an empty list to store gallery images\n",
    "      gallery = []\n",
    "      # Iterate over each path in the gallery paths list\n",
    "      for item in gallery_pths:\n",
    "          # Open each gallery image and append it to the gallery list\n",
    "          gallery.append(Image.open(item))\n",
    "\n",
    "      # Check if a transform is set to be applied to the images\n",
    "      if self.transform:\n",
    "          # Apply the transform to the probe image\n",
    "          probe = self.transform(probe)\n",
    "\n",
    "          # Apply the transform to each image in the gallery list\n",
    "          for i in range(len(gallery)):\n",
    "              gallery[i] = self.transform(gallery[i])\n",
    "\n",
    "      # Return a list containing the probe image, gallery images, target labels, and their respective paths\n",
    "      return [probe, gallery, is_match, probe_pth, gallery_pths, probe_target, gallery_targets]\n",
    "\n",
    "    def read_folder(self):\n",
    "        paths = []  # Initialize a list to store image paths\n",
    "        labels = []  # Initialize a list to store labels\n",
    "\n",
    "        # Retrieve all items in the given root directory\n",
    "        for dirpath, dirnames, filenames in os.walk(self.data_root):\n",
    "            # Filter for images in .jpg, .jpeg, and .png format\n",
    "            files = [f for f in filenames if f.split('.')[-1] in ['jpg', 'jpeg', 'png']]\n",
    "\n",
    "            # Build file paths and labels\n",
    "            for item in files:\n",
    "                label = dirpath.split('/')[-1]  # Extract the label from the directory name (e.g., probe or gallery)\n",
    "                if label not in labels:\n",
    "                    labels.append(label)\n",
    "\n",
    "                paths.append([f'{dirpath}/{item}', label])  # Append the file path and label (e.g., gallery or probe)\n",
    "        return paths\n",
    "\n",
    "    def create_samples(self):\n",
    "        # Create a list of samples for the dataset\n",
    "        probes = [x[0] for x in self.data if x[1] == 'Probe']  # Get paths of images labeled as 'Probe'\n",
    "        gallery = [x[0] for x in self.data if x[1] == 'Gallery']  # Get paths of images labeled as 'Gallery'\n",
    "\n",
    "        samples = []\n",
    "        # Pair each 'Probe' image with every 'Gallery' image and assign a target\n",
    "        for probe in probes:\n",
    "          is_match = []\n",
    "          gallery_targets = []\n",
    "          probe_id = extract_id_from_imagename(probe)  # Extract ID from probe image name\n",
    "          probe_id = int(probe_id[:-2])-1 # IDs range [1, 81] -> [0, 80]\n",
    "\n",
    "          for item in gallery:\n",
    "            item_id = extract_id_from_imagename(item)  # Extract ID from gallery image name\n",
    "            item_id = int(item_id[:-2])-1 # IDs range [1, 81] -> [0, 80]\n",
    "            # Assign a target of 1 if IDs match (ignoring the last two characters), otherwise -1\n",
    "            is_match.append(1 if probe_id == item_id else -1)\n",
    "            gallery_targets.append(item_id)\n",
    "\n",
    "          samples.append([probe, gallery, is_match, probe_id, gallery_targets])\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformation to be applied to the images (converting them to PyTorch tensors)\n",
    "if backbone == 'ViT16':\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "else:\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(128), \n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "# Load the training, validation, and test datasets from specified directories and apply the defined transformation\n",
    "train_dataset = ProbeGalleryTrain('./output/ImageData/D1/', transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = ProbeGalleryValTest('./output/ImageData/D2v/', transform=transform)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = ProbeGalleryValTest('./output/ImageData/D2t/', transform=transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAnwcBGdigiN"
   },
   "source": [
    "# Neural Network Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Siamese network class inheriting from nn.Module\n",
    "class BSN(nn.Module):\n",
    "    def __init__(self, backbone, img_channels, depth, img_w, img_h, n_classes):\n",
    "        super(BSN, self).__init__()\n",
    "\n",
    "        self.backbone = backbone\n",
    "        \n",
    "        if self.backbone == 'siameseNet':\n",
    "            # Initialize convolutional and pooling layers\n",
    "            # First convolutional layer with specified input channels and depth\n",
    "            self.conv1 = nn.Conv2d(img_channels, depth, kernel_size=3, padding=1)\n",
    "            # First pooling layer, reduces spatial dimensions by half\n",
    "            self.pool1 = nn.MaxPool2d(2, 2)\n",
    "            # Second convolutional layer, doubling the depth\n",
    "            self.conv2 = nn.Conv2d(depth, depth*2, kernel_size=3, padding=1)\n",
    "            # Second pooling layer, further reducing spatial dimensions\n",
    "            self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "            # Calculate the output size after the convolutional and pooling layers\n",
    "            output_size = depth*2 * img_h//4 * img_w//4\n",
    "            \n",
    "            \n",
    "\n",
    "        if self.backbone == 'VGG19':\n",
    "            self.model = models.vgg19_bn().features # VGG19 with batch normalization\n",
    "\n",
    "            # Create a dummy input tensor of the correct size\n",
    "            dummy_input = torch.randn(1, img_channels, img_h, img_w)\n",
    "\n",
    "            # Run a forward pass through the convolutional layers\n",
    "            output = self.model(dummy_input)\n",
    "\n",
    "            # Calculate the output size\n",
    "            output_size = output.view(output.size(0), -1).size(1)\n",
    "        \n",
    "        if self.backbone == 'VGG16':\n",
    "            self.model = models.vgg16_bn().features # VGG16 with batch normalization\n",
    "\n",
    "            # Create a dummy input tensor of the correct size\n",
    "            dummy_input = torch.randn(1, img_channels, img_h, img_w)\n",
    "\n",
    "            # Run a forward pass through the convolutional layers\n",
    "            output = self.model(dummy_input)\n",
    "\n",
    "            # Calculate the output size\n",
    "            output_size = output.view(output.size(0), -1).size(1)\n",
    "\n",
    "        if self.backbone == 'denseNet':\n",
    "            self.model = models.densenet121().features # DenseNet121 with batch normalization\n",
    "            \n",
    "            # Create a dummy input tensor of the correct size\n",
    "            dummy_input = torch.randn(1, img_channels, img_h, img_w)\n",
    "\n",
    "            # Run a forward pass through the convolutional layers\n",
    "            output = self.model(dummy_input)\n",
    "\n",
    "            # Calculate the output size\n",
    "            output_size = output.view(output.size(0), -1).size(1)\n",
    "\n",
    "        if self.backbone == \"MobileNetV3\":\n",
    "            self.model = models.mobilenet_v3_small().features # MobileNetV3 with batch normalization\n",
    "\n",
    "            # Create a dummy input tensor of the correct size\n",
    "            dummy_input = torch.randn(1, img_channels, img_h, img_w)\n",
    "\n",
    "            # Run a forward pass through the convolutional layers\n",
    "            output = self.model(dummy_input)\n",
    "\n",
    "            # Calculate the output size\n",
    "            output_size = output.view(output.size(0), -1).size(1)\n",
    "            \n",
    "        if self.backbone == 'efficientNetB0':\n",
    "            self.model = models.efficientnet_b0().features # VGG16 with batch normalization\n",
    "\n",
    "            # Create a dummy input tensor of the correct size\n",
    "            dummy_input = torch.randn(1, img_channels, img_h, img_w)\n",
    "\n",
    "            # Run a forward pass through the convolutional layers\n",
    "            output = self.model(dummy_input)\n",
    "\n",
    "            # Calculate the output size\n",
    "            output_size = output.view(output.size(0), -1).size(1)\n",
    "\n",
    "        if self.backbone == \"ViT16\":\n",
    "            self.model = models.vit_b_16() # ViT16\n",
    "            self.model.heads = nn.Identity() # remove the classification head\n",
    "\n",
    "            # Create a dummy input tensor of the correct size\n",
    "            dummy_input = torch.randn(1, img_channels, 224, 224)\n",
    "\n",
    "            # Run a forward pass through the convolutional layers\n",
    "            output = self.model(dummy_input)\n",
    "\n",
    "            # Calculate the output size\n",
    "            output_size = output.view(output.size(0), -1).size(1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(output_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.classifier = nn.Linear(256,n_classes)\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    # Define the forward pass for one branch of the Siamese network\n",
    "    def forward_one(self, x):\n",
    "        if self.backbone == 'siameseNet':\n",
    "            # Apply the first convolutional layer followed by ReLU activation and pooling\n",
    "            x = self.pool1(F.relu(self.conv1(x)))\n",
    "            # Apply the second convolutional layer, ReLU, and pooling\n",
    "            x = self.pool2(F.relu(self.conv2(x)))\n",
    "            # Flatten the output for the fully connected layer\n",
    "            x = x.view(x.size(0), -1)\n",
    "            # Apply the first fully connected layer with ReLU activation\n",
    "            x = self.dropout(F.relu(self.fc1(x)))\n",
    "            # Apply the second fully connected layer\n",
    "            x = self.dropout(F.relu(self.fc2(x)))\n",
    "            y = self.classifier(x)\n",
    "        else:\n",
    "            # Apply the selected model to the input\n",
    "            x = self.model(x)\n",
    "            # flatten output \n",
    "            x = x.view(x.size(0), -1)\n",
    "            # Apply classifier component\n",
    "            x = self.dropout(F.relu(self.fc1(x)))\n",
    "            x = self.dropout(F.relu(self.fc2(x)))\n",
    "            y = self.classifier(x)\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "    # Define the forward pass for the whole Siamese network\n",
    "    def forward(self, input1, input2):\n",
    "        # Process each input through the network\n",
    "        output1, output_class1 = self.forward_one(input1)\n",
    "        output2, output_class2= self.forward_one(input2)\n",
    "        return output1, output_class1, output2, output_class2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "MbIotu3sgHIb"
   },
   "outputs": [],
   "source": [
    "def compute_ap(matches):\n",
    "    \"\"\"Compute the average precision (AP) given ranks of positive images and the number of positive images.\n",
    "    Args:\n",
    "        matches (list): The ranks of positive images sorted in ascending order.\n",
    "    Returns:\n",
    "        float: The average precision for the given data.\n",
    "    \"\"\"\n",
    "    if len(matches) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    matched = 0\n",
    "    ap = 0.0\n",
    "    \n",
    "    for rank, match in enumerate(matches, start=1):\n",
    "        if match:\n",
    "            matched += 1\n",
    "            ap += matched / rank\n",
    "\n",
    "    return ap / matched\n",
    "\n",
    "def testing_step(model, test_dataloader):\n",
    "    # Switch the model to evaluation mode. This turns off specific layers/features like dropout.\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize counters for correct predictions at different ranks\n",
    "    correct1 = 0  # Correct predictions at rank 1\n",
    "    correct2 = 0\n",
    "    correct3 = 0\n",
    "    correct4 = 0\n",
    "    correct5 = 0  # Correct predictions within the top 5\n",
    "    correct6 = 0\n",
    "    correct7 = 0\n",
    "    correct8 = 0\n",
    "    correct9 = 0\n",
    "    correct10 = 0  # Correct predictions within the top 10\n",
    "    samples = 0  # Total number of samples processed\n",
    "\n",
    "    # Set the rank limit for evaluation\n",
    "    K = 10\n",
    "    AP_scores = []  # To store Average Precision scores for each query\n",
    "    # Initialize an empty list to store the ranking results\n",
    "    rank_matrix =[]\n",
    "\n",
    "    # Disable gradient calculation for efficiency and to prevent changes to the model\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the test dataloader\n",
    "        for data in tqdm(test_dataloader, leave=False, total=len(test_dataloader)):\n",
    "            # Unpack the data\n",
    "            probe, gallery, is_match, probe_pth, gallery_pths, probe_target, gallery_targets = data\n",
    "\n",
    "            # Initialize a list to store similarity scores\n",
    "            scores = []\n",
    "            # Concatenate gallery images into a batch\n",
    "            gallery = torch.cat(gallery, axis=0)\n",
    "\n",
    "            # Repeat the probe image to match the number of gallery images\n",
    "            probe_batch = probe.repeat(gallery.shape[0], 1, 1, 1)\n",
    "\n",
    "            # Get model outputs for the probe and gallery batches\n",
    "            output1, output_class1, output2, output_class2 = model(probe_batch.to(device), gallery.to(device))\n",
    "\n",
    "            # Calculate similarity scores for each pair of probe and gallery images\n",
    "            for i in range(len(probe_batch)):\n",
    "                a = output1[i].detach().cpu().numpy()\n",
    "                b = output2[i].detach().cpu().numpy()\n",
    "                # Compute cosine similarity\n",
    "                cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "                # Append the similarity score and corresponding gallery path\n",
    "                scores.append([cos_sim, gallery_pths[i][0]])\n",
    "            \n",
    "            samples += 1\n",
    "\n",
    "            # Sort the scores in descending order to rank the gallery images\n",
    "            topk = sorted(scores, key=lambda element: (element[0]), reverse=True)\n",
    "            # Store the probe path and its corresponding ranked gallery paths\n",
    "            rank_matrix.append([probe_pth[0], topk])\n",
    "\n",
    "            # Compute the Average Precision (AP) for the query\n",
    "            pos_ranks = []\n",
    "            probe_id = extract_id_from_imagename(probe_pth[0])\n",
    "            probe_id = int(probe_id[:-2]) - 1\n",
    "            for i, score in enumerate(topk):\n",
    "                gallery_id = extract_id_from_imagename(score[1])\n",
    "                gallery_id = int(gallery_id[:-2]) - 1\n",
    "                if probe_id == gallery_id:\n",
    "                    pos_ranks.append(i)\n",
    "            if pos_ranks:\n",
    "                AP = compute_ap(pos_ranks)  # Assuming each probe_id should match with a unique gallery_id\n",
    "                AP_scores.append(AP)  # Assuming each probe is supposed to match with exactly one gallery\n",
    "\n",
    "        # Compute correct predictions for rank 1, 5, and 10\n",
    "        for idx, probe in enumerate(rank_matrix):\n",
    "            # Extract the probe ID\n",
    "            probe_id=extract_id_from_imagename(probe[0])\n",
    "            probe_id = int(probe_id[:-2])-1 # IDs range [1, 81] -> [0, 80]\n",
    "            for i in range(K):\n",
    "                # Extract the gallery ID\n",
    "                gallery_id=extract_id_from_imagename(probe[1][i][1])\n",
    "                gallery_id = int(gallery_id[:-2])-1 # IDs range [1, 81] -> [0, 80]\n",
    "                # Check if the IDs match\n",
    "                if probe_id == gallery_id:\n",
    "                    # Update the correct counters based on the rank\n",
    "                    if i == 0:\n",
    "                        correct1 += 1\n",
    "                        correct2 += 1\n",
    "                        correct3 += 1\n",
    "                        correct4 += 1\n",
    "                        correct5 += 1\n",
    "                        correct6 += 1\n",
    "                        correct7 += 1\n",
    "                        correct8 += 1\n",
    "                        correct9 += 1\n",
    "                        correct10 += 1\n",
    "                    elif i == 1:\n",
    "                        correct2 += 1\n",
    "                        correct3 += 1\n",
    "                        correct4 += 1\n",
    "                        correct5 += 1\n",
    "                        correct6 += 1\n",
    "                        correct7 += 1\n",
    "                        correct8 += 1\n",
    "                        correct9 += 1\n",
    "                        correct10 += 1\n",
    "                    elif i == 2:\n",
    "                        correct3 += 1\n",
    "                        correct4 += 1\n",
    "                        correct5 += 1\n",
    "                        correct6 += 1\n",
    "                        correct7 += 1\n",
    "                        correct8 += 1\n",
    "                        correct9 += 1\n",
    "                        correct10 += 1\n",
    "                    elif i == 3:\n",
    "                        correct4 += 1\n",
    "                        correct5 += 1\n",
    "                        correct6 += 1\n",
    "                        correct7 += 1\n",
    "                        correct8 += 1\n",
    "                        correct9 += 1\n",
    "                        correct10 += 1\n",
    "                    elif i == 4:\n",
    "                        correct5 += 1\n",
    "                        correct6 += 1\n",
    "                        correct7 += 1\n",
    "                        correct8 += 1\n",
    "                        correct9 += 1\n",
    "                        correct10 += 1\n",
    "                    elif i == 5:\n",
    "                        correct6 += 1\n",
    "                        correct7 += 1\n",
    "                        correct8 += 1\n",
    "                        correct9 += 1\n",
    "                        correct10 += 1\n",
    "                    elif i == 6:\n",
    "                        correct7 += 1\n",
    "                        correct8 += 1\n",
    "                        correct9 += 1\n",
    "                        correct10 += 1\n",
    "                    elif i == 7:\n",
    "                        correct8 += 1\n",
    "                        correct9 += 1\n",
    "                        correct10 += 1\n",
    "                    elif i == 8:\n",
    "                        correct9 += 1\n",
    "                        correct10 += 1\n",
    "                    elif i == 9:\n",
    "                        correct10 += 1\n",
    "\n",
    "                    break\n",
    "\n",
    "    # Compute mAP\n",
    "    mAP = np.mean(AP_scores)\n",
    "\n",
    "    # Print the accuracy at different ranks\n",
    "    print(f'Total samples: {samples}, Rank #1: {correct1/samples*100.0:.3f}%, Rank #2: {correct2/samples*100.0:.3f}%, Rank #3: {correct3/samples*100.0:.3f}%, Rank #4: {correct4/samples*100.0:.3f}%, Rank #5: {correct5/samples*100.0:.3f}%, Rank #6: {correct6/samples*100.0:.3f}%, Rank #7: {correct7/samples*100.0:.3f}%, Rank #8: {correct8/samples*100.0:.3f}%, Rank #9: {correct9/samples*100.0:.3f}%, Rank #10: {correct10/samples*100.0:.3f}%, mAP:{mAP*100.0:.3f}%')\n",
    "    return correct1/samples, correct5/samples, correct10/samples, mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 703
    },
    "executionInfo": {
     "elapsed": 898987,
     "status": "error",
     "timestamp": 1702377865815,
     "user": {
      "displayName": "Marco Cascio",
      "userId": "18238624429496429187"
     },
     "user_tz": -60
    },
    "id": "Ulh5_qjgiGAf",
    "outputId": "eaba4842-aece-41b8-af75-95c391216431"
   },
   "outputs": [],
   "source": [
    "def training_model(model, train_dataloader, val_dataloader, optimizer, embg_criterion, cat_criterion, n_epochs):    \n",
    "    # Initialize variable to store the best rank #10 accuracy\n",
    "    bestRank1 = 0.0\n",
    "    bestRank5 = 0.0\n",
    "    bestRank10 = 0.0\n",
    "\n",
    "    # Training loop for a specified number of epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        # Set the network in training mode (this enables features like dropout and batch normalization)\n",
    "        model.train()\n",
    "\n",
    "        # Initialize a variable to accumulate loss over the epoch\n",
    "        train_loss = 0.0\n",
    "\n",
    "        # Iterate over batches in the training data loader\n",
    "        for batch in tqdm(train_dataloader, leave=False, total=len(train_dataloader)):\n",
    "            # Unpack the batch into images and target values and move them to the computation device\n",
    "            img1 = batch[0].to(device)\n",
    "            img2 = batch[1].to(device)\n",
    "            is_match = batch[2].to(device)\n",
    "            img1_target = batch[3].to(device)\n",
    "            img2_target = batch[4].to(device)\n",
    "            \n",
    "            # Forward pass: Compute the output of the network for both images in the pair\n",
    "            output1, output_class1, output2, output_class2 = model(img1, img2)\n",
    "\n",
    "            # Compute the loss based on the outputs and the target values\n",
    "            embg_loss = embg_criterion(output1, output2, is_match)\n",
    "            cat_loss = cat_criterion(output_class1, img1_target) + cat_criterion(output_class2, img2_target)\n",
    "            loss = embg_loss + cat_loss\n",
    "\n",
    "            # Zero the gradients before running the backward pass\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Backward pass: Compute gradient of the loss with respect to network parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # Perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate the loss over the batches\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Perform validation step and return the rank #10 accuracy for this epoch\n",
    "        rank1, rank5, rank10, _ = testing_step(model, val_dataloader)\n",
    "\n",
    "        # Folder to save model weights\n",
    "        models_folder = os.path.join('./output/models/')\n",
    "        if not os.path.exists(models_folder):\n",
    "            os.makedirs(models_folder, exist_ok=True)\n",
    "\n",
    "        # Check if the current model is the best so far; if so, save it and update the best rank #1 accuracy\n",
    "        if rank1 > bestRank1:\n",
    "            torch.save(model.state_dict(), f'{models_folder}/best_{backbone}_state_r1.bin')\n",
    "            bestRank1 = rank1\n",
    "        \n",
    "        # Check if the current model is the best so far; if so, save it and update the best rank #5 accuracy\n",
    "        if rank5 > bestRank5:\n",
    "            torch.save(model.state_dict(), f'{models_folder}/best_{backbone}_state_r5.bin')\n",
    "            bestRank5 = rank5\n",
    "\n",
    "        # Check if the current model is the best so far; if so, save it and update the best rank #10 accuracy\n",
    "        if rank10 > bestRank10:\n",
    "            torch.save(model.state_dict(), f'{models_folder}/best_{backbone}_state_r10.bin')\n",
    "            bestRank10 = rank10\n",
    "\n",
    "        torch.save(model.state_dict(), f'{models_folder}/best_{backbone}_epoch_'+str(epoch)+'.bin')\n",
    "\n",
    "        # Compute the average loss for this epoch and print it\n",
    "        running_loss = train_loss / len(train_dataloader)\n",
    "        print(f\"Epoch: [{epoch+1}/{n_epochs}]| Loss: {running_loss:.5f}\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Siamese network with the specified parameters and move it to the selected device\n",
    "model = BSN(backbone, img_channels, depth, img_w, img_h, n_classes).to(device)\n",
    "\n",
    "# Define the loss function for training the network (Cosine Embedding Loss)\n",
    "embg_criterion = nn.CosineEmbeddingLoss()\n",
    "cat_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer for training, using the Adam algorithm with a learning rate of 0.0005\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 36, Rank #1: 52.778%, Rank #2: 63.889%, Rank #3: 75.000%, Rank #4: 91.667%, Rank #5: 91.667%, Rank #6: 91.667%, Rank #7: 91.667%, Rank #8: 91.667%, Rank #9: 91.667%, Rank #10: 91.667%, mAP:0.736%\n",
      "Epoch: [1/1]| Loss: 4.14861\n"
     ]
    }
   ],
   "source": [
    "# Start training model\n",
    "training_model(model, train_dataloader, val_dataloader, optimizer, embg_criterion, cat_criterion, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model w/ best R1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 36, Rank #1: 63.889%, Rank #2: 66.667%, Rank #3: 75.000%, Rank #4: 83.333%, Rank #5: 86.111%, Rank #6: 88.889%, Rank #7: 88.889%, Rank #8: 88.889%, Rank #9: 91.667%, Rank #10: 91.667%, mAP:0.734%\n",
      "Model w/ best R5:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 36, Rank #1: 63.889%, Rank #2: 66.667%, Rank #3: 75.000%, Rank #4: 83.333%, Rank #5: 86.111%, Rank #6: 88.889%, Rank #7: 88.889%, Rank #8: 88.889%, Rank #9: 91.667%, Rank #10: 91.667%, mAP:0.734%\n",
      "Model w/ best R10:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 36, Rank #1: 63.889%, Rank #2: 66.667%, Rank #3: 75.000%, Rank #4: 83.333%, Rank #5: 86.111%, Rank #6: 88.889%, Rank #7: 88.889%, Rank #8: 88.889%, Rank #9: 91.667%, Rank #10: 91.667%, mAP:0.734%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6388888888888888,\n",
       " 0.8611111111111112,\n",
       " 0.9166666666666666,\n",
       " 0.7337962962962962)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize three Siamese neural network models with the same architecture and parameters\n",
    "m1 = BSN(backbone, img_channels, depth, img_w, img_h, n_classes).to(device)\n",
    "m5 = BSN(backbone, img_channels, depth, img_w, img_h, n_classes).to(device)\n",
    "m10 = BSN(backbone, img_channels, depth, img_w, img_h, n_classes).to(device)\n",
    "\n",
    "# Load pre-trained model weights for each of the three models\n",
    "m1.load_state_dict(torch.load('./output/models/best_'+backbone+'_state_r1.bin'))\n",
    "m5.load_state_dict(torch.load('./output/models/best_'+backbone+'_state_r5.bin'))\n",
    "m10.load_state_dict(torch.load('./output/models/best_'+backbone+'_state_r10.bin'))\n",
    "\n",
    "# Evaluate and print the performance of each model with their respective evaluation metric\n",
    "print('Model w/ best R1:')\n",
    "testing_step(m1, test_dataloader)\n",
    "print('Model w/ best R5:')\n",
    "testing_step(m5, test_dataloader)\n",
    "print('Model w/ best R10:')\n",
    "testing_step(m10, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model w/ best R1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 243, Rank #1: 48.971%, Rank #2: 58.025%, Rank #3: 65.844%, Rank #4: 72.016%, Rank #5: 75.309%, Rank #6: 79.424%, Rank #7: 82.716%, Rank #8: 83.539%, Rank #9: 84.774%, Rank #10: 85.597%, mAP:79.595%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4897119341563786,\n",
       " 0.7530864197530864,\n",
       " 0.8559670781893004,\n",
       " 0.7959533607681756)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize three Siamese neural network models with the same architecture and parameters\n",
    "m1 = BSN(backbone, img_channels, depth, img_w, img_h, n_classes).to(device)\n",
    "\n",
    "################## 1-shot ##################\n",
    "# Load the state dictionary from the file\n",
    "state_dict = torch.load('./output/models/best_'+backbone+'_state_r1.bin')\n",
    "\n",
    "# Create a new state dictionary for the model\n",
    "new_state_dict = {}\n",
    "\n",
    "# Map each key in the state dictionary to the new key in your model\n",
    "for key in state_dict:\n",
    "    # Replace 'model' with 'backbone' in each key\n",
    "    new_key = key.replace('backbone', 'model')\n",
    "    # Assign the weight to the new key in the new state dictionary\n",
    "    new_state_dict[new_key] = state_dict[key]\n",
    "\n",
    "# Load the new state dictionary into your model\n",
    "m1.load_state_dict(new_state_dict)\n",
    "\n",
    "# Evaluate and print the performance of each model with their respective evaluation metric\n",
    "print('Model w/ best R1:')\n",
    "testing_step(m1, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "2023-WiFiDataset-ECCV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
